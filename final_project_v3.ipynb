{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Christopher Morales\n",
    "## EE 5830 - Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.10.12)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "# Importing Libraries\n",
    "import pygame\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants for the DQN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Screen Resolution\n",
    "WIDTH, HEIGHT = 600, 400\n",
    "\n",
    "# How many grids to have (x by y)\n",
    "GRID_SIZE = 20\n",
    "\n",
    "# Frames Per Second\n",
    "FPS = 60\n",
    "\n",
    "# The size of the replay memory used in experience replay (to store and sample past experience)\n",
    "REPLAY_MEMORY_SIZE = 10000\n",
    "\n",
    "# The number of samples (transitions) randomly sampled from the replay memory\n",
    "BATCH_SIZE = 1000\n",
    "\n",
    "# Lower gamma makes the agent focus on immediate rewards where higher gamma considered future rewards more\n",
    "GAMMA = 0.95\n",
    "\n",
    "# Higher the value then explore (exploration trade off parameter)\n",
    "EPSILON_START = 1.0\n",
    "\n",
    "# Sets the minimum value that epsilon can reach (exploration)\n",
    "EPSILON_END = 0.01\n",
    "\n",
    "# To allow the agent to tansistion from exploration to exploitation as it learns ()\n",
    "EPSILON_DECAY = 0.99\n",
    "\n",
    "# Number of episodes (training time essentially)\n",
    "MAX_EPISODE_VALUE = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Transistion state (how the model can learn from past and current)\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Snake Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Snake:\n",
    "    def __init__(self):\n",
    "        # Set initial position in the grid\n",
    "        self.position = [GRID_SIZE * 2, GRID_SIZE * 2] \n",
    "        \n",
    "        # Initialize the snake's body with three segments, including the initial position\n",
    "        self.body = [list(self.position), [self.position[0] - GRID_SIZE, self.position[1]], [self.position[0] - 2 * GRID_SIZE, self.position[1]]]\n",
    "        \n",
    "        # Set the initial direction of the snake to 'RIGHT'\n",
    "        self.direction = 'RIGHT'\n",
    "        \n",
    "        # Set the initial change direction to the current direction\n",
    "        self.change_to = self.direction\n",
    "\n",
    "    def change_direction(self, new_direction):\n",
    "        \"\"\"\n",
    "        Gets the model action input to determine what direction the snake should go\n",
    "\n",
    "        :param new_direction: an integer value that will be represented as UP, DOWN, RIGHT, or LEFT\n",
    "        return: N/A\n",
    "        \"\"\"\n",
    "        # Checks if the new_direction is an integer\n",
    "        if isinstance(new_direction, int):\n",
    "            # If the value is 1 then\n",
    "            if new_direction == 1:\n",
    "                # Move RIGHT\n",
    "                self.direction = 'RIGHT'\n",
    "                \n",
    "                # Verifying the condition input\n",
    "                print(\"Moving right\")\n",
    "            \n",
    "            # If the value is 1 then\n",
    "            elif new_direction == 2:\n",
    "                # Move LEFT\n",
    "                self.direction = 'LEFT'\n",
    "\n",
    "                # Verifying the condition input\n",
    "                print(\"Moving left\")\n",
    "            \n",
    "            # If the value is 1 then\n",
    "            elif new_direction == 3:\n",
    "                # Move UP\n",
    "                self.direction = 'UP'\n",
    "                \n",
    "                # Verifying the condition input\n",
    "                print(\"Moving up\")\n",
    "            \n",
    "            # If the value is 1 then\n",
    "            elif new_direction == 4:\n",
    "                # Move DOWN\n",
    "                self.direction = 'DOWN'\n",
    "                \n",
    "                # Verifying the condition input\n",
    "                print(\"Moving down\")\n",
    "\n",
    "            # If the value is something else\n",
    "            else:\n",
    "                # Verifying the condition input\n",
    "                print(\"Invalid direction\")\n",
    "        \n",
    "        # If the new_direction is a different data type somehow\n",
    "        else:\n",
    "            print(\"Invalid direction format. Expected integer.\")\n",
    "\n",
    "\n",
    "    def move(self, pebble):\n",
    "        \"\"\"\n",
    "        Move the snake in the current direction, update its position and body.\n",
    "\n",
    "        :param pebble: Pebble object representing the food for the snake\n",
    "        :return: True if the snake ate the pebble and False otherwise\n",
    "        \"\"\"\n",
    "        # Move the snake to the right\n",
    "        if self.direction == 'RIGHT':\n",
    "            # Update the x-coordinate of the snake's position\n",
    "            self.position[0] += GRID_SIZE\n",
    "        \n",
    "        # Move the snake to the left\n",
    "        elif self.direction == 'LEFT':\n",
    "            # Update the x-coordinate of the snake's position\n",
    "            self.position[0] -= GRID_SIZE\n",
    "        \n",
    "        # Move the snake upwards\n",
    "        elif self.direction == 'UP':\n",
    "            # Update the y-coordinate of the snake's position\n",
    "            self.position[1] -= GRID_SIZE\n",
    "        \n",
    "        # Move the snake downwards\n",
    "        elif self.direction == 'DOWN':\n",
    "            # Update the y-coordinate of the snake's position\n",
    "            self.position[1] += GRID_SIZE\n",
    "\n",
    "        # Update the body positions\n",
    "        self.body.insert(0, list(self.position))\n",
    "        \n",
    "        # Check if the snake's position coincides with the pebble's position\n",
    "        if self.position == pebble.position:\n",
    "            # Snake ate the pebble, grow the body\n",
    "            return True  \n",
    "        \n",
    "        # If the snake did not eat the pebble\n",
    "        else:\n",
    "            # Remove the last segment\n",
    "            self.body.pop()\n",
    "\n",
    "            # Snake did not eat the pebble\n",
    "            return False  \n",
    "\n",
    "    def check_collision(self):\n",
    "        \"\"\"\n",
    "        Check if the snake has collided with the border or itself.\n",
    "\n",
    "        :return: True if collision occurred, False otherwise\n",
    "        \"\"\"\n",
    "        # Check if the snake's x-coordinate is outside the game window\n",
    "        if (\n",
    "            self.position[0] >= WIDTH\n",
    "            or self.position[0] < 0\n",
    "            or self.position[1] >= HEIGHT\n",
    "            or self.position[1] < 0\n",
    "        ):\n",
    "            # Snake collided with the border\n",
    "            return True  \n",
    "        \n",
    "        # Check if the snake collided with itself\n",
    "        for segment in self.body[1:]:\n",
    "            # Compare each body segment with the snake's current position\n",
    "            if segment == self.position:\n",
    "                # Snake collided with itself\n",
    "                return True  \n",
    "        # If no collision occurred\n",
    "        return False\n",
    "\n",
    "    def get_head_position(self):\n",
    "        \"\"\"\n",
    "        Get the position of the snake's head.\n",
    "\n",
    "        :return: List representing the x and y coordinates of the head position\n",
    "        \"\"\"\n",
    "        # Return the current position of the snake's head\n",
    "        return self.position\n",
    "\n",
    "    def get_body_positions(self):\n",
    "        \"\"\"\n",
    "        Get the positions of all segments in the snake's body.\n",
    "\n",
    "        :return: List of lists representing x and y coordinates of each body segment\n",
    "        \"\"\"\n",
    "        # Return the positions of all segments in the snake's body\n",
    "        return self.body\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pebble Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pebble:\n",
    "    def __init__(self):\n",
    "        # Initialize the pebble's position randomly within the grid\n",
    "        self.position = [random.randrange(1, (WIDTH//GRID_SIZE)) * GRID_SIZE,\n",
    "                         random.randrange(1, (HEIGHT//GRID_SIZE)) * GRID_SIZE]\n",
    "\n",
    "    def respawn(self):\n",
    "        \"\"\"\n",
    "        Respawn the pebble at a new random position within the grid.\n",
    "        \n",
    "        return: None\n",
    "        \"\"\"\n",
    "        # Set the pebble's position to a new random position within the grid\n",
    "        self.position = [random.randrange(1, (WIDTH//GRID_SIZE)) * GRID_SIZE,\n",
    "                         random.randrange(1, (HEIGHT//GRID_SIZE)) * GRID_SIZE]\n",
    "\n",
    "    def get_position(self):\n",
    "        \"\"\"\n",
    "        Get the current position of the pebble.\n",
    "        \n",
    "        return: List representing the pebble's position [x, y]\n",
    "        \"\"\"\n",
    "        # Return the current position of the pebble\n",
    "        return self.position\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDQNModel(nn.Module):\n",
    "    def __init__(self, n_observation, n_actions):\n",
    "        # Initialize the DQN model as a subclass of nn.Module\n",
    "        super(MyDQNModel, self).__init__()\n",
    "        # Define the first fully connected layer with input size 12 and output size 64\n",
    "        self.function_one = nn.Linear(12, 64)\n",
    "\n",
    "        # Define the second fully connected layer with input size 64 and output size 64\n",
    "        self.function_two = nn.Linear(64, 64)\n",
    "\n",
    "        # Define the third fully connected layer with input size 64 and output size n_actions\n",
    "        # (representing the possible actions the model can take)\n",
    "        self.function_three = nn.Linear(64, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the DQN model.\n",
    "\n",
    "        :param x: Input tensor representing the state of the environment.\n",
    "        return: Output tensor representing the Q-values for each possible action.\n",
    "        \"\"\"\n",
    "        # Apply ReLU activation to the output of the first fully connected layer\n",
    "        x = torch.relu(self.function_one(x))\n",
    "\n",
    "        # Apply ReLU activation to the output of the second fully connected layer\n",
    "        x = torch.relu(self.function_two(x))\n",
    "\n",
    "        # Output the Q-values for each possible action without activation\n",
    "        return self.function_three(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Memory Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    def __init__(self):\n",
    "        # Initialize an empty list to store replay memory transitions\n",
    "        self.memory = []\n",
    "\n",
    "        # Initialize the position in the memory buffer\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"\n",
    "        Add a transition to the replay memory.\n",
    "\n",
    "        :param *args: A tuple representing a transition (state, action, next_state, reward).\n",
    "        return: None\n",
    "        \"\"\"\n",
    "        # If the memory size is less than the maximum allowed size, append None to the memory list\n",
    "        if len(self.memory) < REPLAY_MEMORY_SIZE:\n",
    "            self.memory.append(None)\n",
    "\n",
    "        # Store the transition at the current position in the memory buffer\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "\n",
    "        # Update the position in the memory buffer using modulo to create a circular buffer\n",
    "        self.position = (self.position + 1) % REPLAY_MEMORY_SIZE\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"\n",
    "        Randomly sample a batch of transitions from the replay memory.\n",
    "\n",
    "        :param batch_size: The number of transitions to sample.\n",
    "        return: A list of sampled transitions.\n",
    "        \"\"\"\n",
    "        # Use random.sample to randomly select a batch of transitions from the memory\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the current size of the replay memory.\n",
    "\n",
    "        return: The number of stored transitions in the replay memory.\n",
    "        \"\"\"\n",
    "        # Return the length of the memory list\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Snake Population Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SnakePopulationAgent:\n",
    "    def __init__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Game Environment Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GameEnvironment:\n",
    "    def __init__(self):\n",
    "        # Initialize the snake object\n",
    "        self.snake = Snake()\n",
    "\n",
    "        # Initialize the pebble object\n",
    "        self.pebble = Pebble()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the game environment by creating a new snake and respawning the pebble.\n",
    "\n",
    "        return: The initial state of the game.\n",
    "        \"\"\"\n",
    "        # Create a new snake\n",
    "        self.snake = Snake()\n",
    "\n",
    "        # Respawn the pebble\n",
    "        self.pebble.respawn()\n",
    "\n",
    "        # Return the initial state of the game\n",
    "        return self.get_state()\n",
    "\n",
    "    def get_state(self):\n",
    "        \"\"\"\n",
    "        Get the current state of the game.\n",
    "\n",
    "        return: A tensor representing the current state of the game.\n",
    "        \"\"\"\n",
    "        # Check if the snake has collided, return a consistent representation for the terminal state\n",
    "        if self.snake.check_collision():\n",
    "            return torch.zeros((1, 12), dtype=torch.float32)\n",
    "        \n",
    "        else:\n",
    "            # Extract relevant information about the state\n",
    "            state = [\n",
    "                self.snake.position[0] / WIDTH,\n",
    "                self.snake.position[1] / HEIGHT,\n",
    "                self.pebble.position[0] / WIDTH,\n",
    "                self.pebble.position[1] / HEIGHT,\n",
    "            ]\n",
    "\n",
    "            # Include body segments in the state representation\n",
    "            for segment in self.snake.body:\n",
    "                state.extend([segment[0] / WIDTH, segment[1] / HEIGHT])\n",
    "\n",
    "            # Pad the state with zeros if needed to match the expected input size\n",
    "            while len(state) < 12:\n",
    "                state.append(0.0)\n",
    "\n",
    "            # Return the state as a tensor\n",
    "            return torch.tensor(state, dtype=torch.float32).view(1, -1)\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Take a step in the environment based on the given action.\n",
    "\n",
    "        :param action: The action to be taken by the snake.\n",
    "        return: The next state and the reward obtained from the step.\n",
    "        \"\"\"\n",
    "        # Change the snake's direction based on the action\n",
    "        self.snake.change_direction(action)\n",
    "\n",
    "        # Move the snake and check if it ate the pebble\n",
    "        pebble_eaten = self.snake.move(self.pebble)\n",
    "\n",
    "        # Handle rewards based on the game state\n",
    "        if pebble_eaten:\n",
    "            reward = 1.0\n",
    "            self.pebble.respawn()\n",
    "\n",
    "        elif self.snake.check_collision():\n",
    "            reward = -1.0\n",
    "            self.reset()\n",
    "        \n",
    "        else:\n",
    "            reward = 0.0\n",
    "\n",
    "        # Get the next state after the step\n",
    "        next_state = self.get_state()\n",
    "\n",
    "        # Return the next state and the obtained reward\n",
    "        return next_state, reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment GUI Version Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvironmentGUIVersion:\n",
    "    def __init__(self, model=None, n_observations=12):\n",
    "        # Initialize Pygame\n",
    "        pygame.init()\n",
    "        \n",
    "        # Pygame clock for controlling frame rate\n",
    "        self.clock = pygame.time.Clock()\n",
    "        \n",
    "        # Initialize the game environment\n",
    "        self.game_env = GameEnvironment()\n",
    "        \n",
    "        # Set up the Pygame screen\n",
    "        self.screen = pygame.display.set_mode((WIDTH, HEIGHT))\n",
    "        \n",
    "        # Set up the font for displaying the score\n",
    "        self.font = pygame.font.SysFont(None, 25)\n",
    "        \n",
    "        # Number of possible actions (UP, DOWN, LEFT, RIGHT)\n",
    "        self.n_actions = 4\n",
    "        \n",
    "        # Use the provided model or create a new one\n",
    "        self.model = model if isinstance(model, MyDQNModel) else MyDQNModel(n_observations, self.n_actions)\n",
    "        \n",
    "        # Exploration-exploitation trade-off parameter\n",
    "        self.epsilon = EPSILON_START\n",
    "        \n",
    "        # Initialize optimizer if a model is provided\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001) if self.model is not None else None\n",
    "        \n",
    "        # Loss functions for training the model\n",
    "        self.mse_loss = nn.MSELoss() if self.model is not None else None\n",
    "        self.mae_loss = nn.L1Loss() if self.model is not None else None\n",
    "        self.smooth_L1_loss = nn.SmoothL1Loss() if self.model is not None else None\n",
    "        self.huber_loss = nn.HuberLoss(reduction='mean', delta=1.0) if self.model is not None else None\n",
    "        \n",
    "        # Replay memory for experience replay\n",
    "        self.memory = ReplayMemory()\n",
    "\n",
    "    def handle_events(self):\n",
    "        \"\"\"\n",
    "        Handle Pygame events, e.g., window close.\n",
    "        \"\"\"\n",
    "        # Iterate through all Pygame events\n",
    "        for event in pygame.event.get():\n",
    "            # Check if the user closed the window\n",
    "            if event.type == pygame.QUIT:\n",
    "                # Quit Pygame and exit the program\n",
    "                pygame.quit()\n",
    "                quit()\n",
    "\n",
    "        # Use the lambda function to get the state for the model\n",
    "        state = (lambda env: env.get_state())(self.game_env)\n",
    "\n",
    "        # Get the action predicted by the model\n",
    "        with torch.no_grad():\n",
    "            action = self.model(state).argmax().item()\n",
    "\n",
    "        # Print the action predicted by the model\n",
    "        print(f\"Model's Action: {action}\")\n",
    "        \n",
    "        # Change the snake's direction based on the predicted action\n",
    "        self.game_env.snake.change_direction(action)\n",
    "    \n",
    "\n",
    "    def draw_snake(self):\n",
    "        \"\"\"\n",
    "        Draw the snake on the Pygame screen.\n",
    "        \"\"\"\n",
    "        # Iterate through all snake body segments\n",
    "        for segment in self.game_env.snake.get_body_positions():\n",
    "            # Draw a green rectangle for each snake segment\n",
    "            pygame.draw.rect(self.screen, (0, 255, 0), pygame.Rect(segment[0], segment[1], GRID_SIZE, GRID_SIZE))\n",
    "\n",
    "    def draw_pebble(self):\n",
    "        \"\"\"\n",
    "        Draw the pebble on the Pygame screen.\n",
    "        \"\"\"\n",
    "        # Get the pebble's position\n",
    "        position = self.game_env.pebble.get_position()\n",
    "        \n",
    "        # Draw a red rectangle for the pebble\n",
    "        pygame.draw.rect(self.screen, (255, 0, 0), pygame.Rect(position[0], position[1], GRID_SIZE, GRID_SIZE))\n",
    "\n",
    "    def draw_score(self, score):\n",
    "        \"\"\"\n",
    "        Draw the current score on the Pygame screen.\n",
    "        \n",
    "        :param score: The current score to display.\n",
    "        \"\"\"\n",
    "        # Render the score text\n",
    "        score_text = self.font.render(f'Score: {score}', True, (255, 255, 255))\n",
    "        \n",
    "        # Blit the score text onto the screen\n",
    "        self.screen.blit(score_text, (10, 10))\n",
    "\n",
    "    def draw_grid(self):\n",
    "        \"\"\"\n",
    "        Draw the grid lines on the Pygame screen.\n",
    "        \"\"\"\n",
    "        # Draw vertical grid lines\n",
    "        for x in range(0, WIDTH, GRID_SIZE):\n",
    "            pygame.draw.line(self.screen, (50, 50, 50), (x, 0), (x, HEIGHT))\n",
    "        \n",
    "        # Draw horizontal grid lines\n",
    "        for y in range(0, HEIGHT, GRID_SIZE):\n",
    "            pygame.draw.line(self.screen, (50, 50, 50), (0, y), (WIDTH, y))\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Main loop for running the environment.\n",
    "        \"\"\"\n",
    "        # Flag indicating whether the game is running\n",
    "        running = True\n",
    "\n",
    "        # Main loop iterating over episodes\n",
    "        for episode in range(1, MAX_EPISODE_VALUE):\n",
    "            # Reset the game environment for a new episode\n",
    "            state = self.game_env.reset()\n",
    "            \n",
    "            # Total reward accumulated during the episode\n",
    "            total_reward = 0\n",
    "\n",
    "            # Episode loop\n",
    "            while True:\n",
    "                # Handle events outside the main loop\n",
    "                self.handle_events()  \n",
    "\n",
    "                # Exploration-exploitation strategy\n",
    "                if random.random() < self.epsilon:\n",
    "                    # Explore: Choose a random action\n",
    "                    action = random.randint(0, 3)\n",
    "                \n",
    "                else:\n",
    "                    # Exploit: Choose the action with the highest Q-value\n",
    "                    with torch.no_grad():\n",
    "                        action = self.model(state).argmax().item()\n",
    "\n",
    "                # Take a step in the environment\n",
    "                next_state, reward = self.game_env.step(action)\n",
    "                \n",
    "                # Update total reward\n",
    "                total_reward += reward\n",
    "\n",
    "                # Store the experience in the replay memory\n",
    "                self.memory.push(state, action, next_state, reward)\n",
    "                \n",
    "                # Update the current state\n",
    "                state = next_state\n",
    "\n",
    "                # Train the model if enough experiences are stored in the memory\n",
    "                if len(self.memory) > BATCH_SIZE:\n",
    "                    # Sample a batch of experiences from the replay memory\n",
    "                    batch = Transition(*zip(*self.memory.sample(BATCH_SIZE)))\n",
    "                    \n",
    "                    # Create a mask indicating non-final next states\n",
    "                    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)),\n",
    "                                                  dtype=torch.bool)\n",
    "                    \n",
    "                    # Determine the maximum length among next states\n",
    "                    max_len = max(s.shape[1] if s is not None else 0 for s in batch.next_state)\n",
    "\n",
    "                    # Pad non-final next states to have the same length\n",
    "                    non_final_next_states = torch.cat([\n",
    "                        torch.nn.functional.pad(s, (0, max_len - s.shape[1])) if s is not None else torch.zeros(1, max_len)\n",
    "                        for s in batch.next_state\n",
    "                    ])\n",
    "\n",
    "                    # Correct input size\n",
    "                    input_size = len(self.game_env.get_state()[0])\n",
    "                    \n",
    "                    # Create a new model with the correct input size\n",
    "                    self.model = MyDQNModel(input_size, self.n_actions)\n",
    "                    \n",
    "                    # Convert the batch data to tensors\n",
    "                    state_batch = torch.cat(batch.state)\n",
    "                    action_batch = torch.tensor(batch.action, dtype=torch.long).view(-1, 1)\n",
    "                    reward_batch = torch.tensor(batch.reward, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "                    # Compute Q-values for the current state and selected actions\n",
    "                    Q_current = self.model(state_batch).gather(1, action_batch)\n",
    "                    \n",
    "                    # Initialize tensor for Q-values of next states\n",
    "                    Q_next = torch.zeros(BATCH_SIZE)\n",
    "\n",
    "                    # Update Q-values for non-final next states\n",
    "                    Q_next[non_final_mask] = self.model(non_final_next_states).max(1)[0].detach()\n",
    "                    \n",
    "                    # Compute Q-target values for the Bellman equation\n",
    "                    Q_target = reward_batch + (GAMMA * Q_next)\n",
    "\n",
    "                    # Compute the Huber loss between current and target Q-values\n",
    "                    loss = self.huber_loss(Q_current, Q_target)\n",
    "                    \n",
    "                    # Zero gradients, perform a backward pass, and update the weights\n",
    "                    self.optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "\n",
    "                # Decay epsilon for exploration-exploitation\n",
    "                self.epsilon = max(EPSILON_END, self.epsilon * EPSILON_DECAY)\n",
    "                print(f\"Episode {episode}, Total Reward: {total_reward}, action state: {action}\")\n",
    "\n",
    "                # Update Pygame screen\n",
    "                self.screen.fill((0, 0, 0))\n",
    "                self.draw_snake()\n",
    "                self.draw_pebble()\n",
    "                self.draw_score(total_reward)\n",
    "                self.draw_grid()\n",
    "                pygame.display.flip()\n",
    "\n",
    "                # Cap the frame rate\n",
    "                self.clock.tick(FPS)\n",
    "\n",
    "                # Exit the loop if the environment is no longer running\n",
    "                if not running:\n",
    "                    pygame.quit()\n",
    "                    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an instance for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the input size based on the state representation in the environment\n",
    "input_size = len(EnvironmentGUIVersion().game_env.get_state()[0])\n",
    "\n",
    "# Assuming 4 possible actions (UP, DOWN, LEFT, RIGHT)\n",
    "n_actions = 4  \n",
    "\n",
    "# Create an instance of the MyDQNModel class with the calculated input size and number of actions\n",
    "model = MyDQNModel(input_size, n_actions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Cell (Combining everything)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: 0.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: 0.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Moving left\n",
      "Episode 1, Total Reward: -1.0, action state: 2\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -1.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Moving up\n",
      "Episode 1, Total Reward: -1.0, action state: 3\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Moving right\n",
      "Episode 1, Total Reward: -1.0, action state: 1\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Moving right\n",
      "Episode 1, Total Reward: -1.0, action state: 1\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Moving right\n",
      "Episode 1, Total Reward: -1.0, action state: 1\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Moving up\n",
      "Episode 1, Total Reward: -1.0, action state: 3\n",
      "Model's Action: 2\n",
      "Moving left\n",
      "Moving left\n",
      "Episode 1, Total Reward: -1.0, action state: 2\n",
      "Model's Action: 2\n",
      "Moving left\n",
      "Moving up\n",
      "Episode 1, Total Reward: -2.0, action state: 3\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Moving right\n",
      "Episode 1, Total Reward: -2.0, action state: 1\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Moving right\n",
      "Episode 1, Total Reward: -2.0, action state: 1\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -2.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Moving up\n",
      "Episode 1, Total Reward: -2.0, action state: 3\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Moving left\n",
      "Episode 1, Total Reward: -2.0, action state: 2\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Moving left\n",
      "Episode 1, Total Reward: -2.0, action state: 2\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -2.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Moving up\n",
      "Episode 1, Total Reward: -2.0, action state: 3\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -3.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -3.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -3.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Moving up\n",
      "Episode 1, Total Reward: -3.0, action state: 3\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -3.0, action state: 0\n",
      "Model's Action: 2\n",
      "Moving left\n",
      "Moving left\n",
      "Episode 1, Total Reward: -3.0, action state: 2\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Moving up\n",
      "Episode 1, Total Reward: -4.0, action state: 3\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -4.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Moving left\n",
      "Episode 1, Total Reward: -5.0, action state: 2\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Moving right\n",
      "Episode 1, Total Reward: -5.0, action state: 1\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -5.0, action state: 0\n",
      "Model's Action: 2\n",
      "Moving left\n",
      "Moving up\n",
      "Episode 1, Total Reward: -5.0, action state: 3\n",
      "Model's Action: 2\n",
      "Moving left\n",
      "Moving up\n",
      "Episode 1, Total Reward: -5.0, action state: 3\n",
      "Model's Action: 2\n",
      "Moving left\n",
      "Moving left\n",
      "Episode 1, Total Reward: -5.0, action state: 2\n",
      "Model's Action: 2\n",
      "Moving left\n",
      "Moving up\n",
      "Episode 1, Total Reward: -6.0, action state: 3\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -6.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Moving left\n",
      "Episode 1, Total Reward: -7.0, action state: 2\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Moving right\n",
      "Episode 1, Total Reward: -7.0, action state: 1\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -7.0, action state: 0\n",
      "Model's Action: 2\n",
      "Moving left\n",
      "Moving left\n",
      "Episode 1, Total Reward: -8.0, action state: 2\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Moving left\n",
      "Episode 1, Total Reward: -9.0, action state: 2\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Moving right\n",
      "Episode 1, Total Reward: -9.0, action state: 1\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -9.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -9.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Moving up\n",
      "Episode 1, Total Reward: -9.0, action state: 3\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -9.0, action state: 0\n",
      "Model's Action: 2\n",
      "Moving left\n",
      "Moving right\n",
      "Episode 1, Total Reward: -9.0, action state: 1\n",
      "Model's Action: 2\n",
      "Moving left\n",
      "Moving left\n",
      "Episode 1, Total Reward: -10.0, action state: 2\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -10.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Moving left\n",
      "Episode 1, Total Reward: -11.0, action state: 2\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -11.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Moving up\n",
      "Episode 1, Total Reward: -11.0, action state: 3\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Moving left\n",
      "Episode 1, Total Reward: -11.0, action state: 2\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -11.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Moving right\n",
      "Episode 1, Total Reward: -12.0, action state: 1\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Moving right\n",
      "Episode 1, Total Reward: -12.0, action state: 1\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Moving up\n",
      "Episode 1, Total Reward: -12.0, action state: 3\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Moving right\n",
      "Episode 1, Total Reward: -12.0, action state: 1\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Moving up\n",
      "Episode 1, Total Reward: -12.0, action state: 3\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -13.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Moving left\n",
      "Episode 1, Total Reward: -14.0, action state: 2\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -14.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -14.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -14.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Moving right\n",
      "Episode 1, Total Reward: -14.0, action state: 1\n",
      "Model's Action: 2\n",
      "Moving left\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -15.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -15.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -15.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Moving left\n",
      "Episode 1, Total Reward: -16.0, action state: 2\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Moving right\n",
      "Episode 1, Total Reward: -16.0, action state: 1\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Moving right\n",
      "Episode 1, Total Reward: -16.0, action state: 1\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Moving up\n",
      "Episode 1, Total Reward: -16.0, action state: 3\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -16.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -17.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -17.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -17.0, action state: 0\n",
      "Model's Action: 2\n",
      "Moving left\n",
      "Moving left\n",
      "Episode 1, Total Reward: -18.0, action state: 2\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -18.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -18.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -18.0, action state: 0\n",
      "Model's Action: 2\n",
      "Moving left\n",
      "Moving left\n",
      "Episode 1, Total Reward: -19.0, action state: 2\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -19.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -19.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Moving right\n",
      "Episode 1, Total Reward: -19.0, action state: 1\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Moving right\n",
      "Episode 1, Total Reward: -19.0, action state: 1\n",
      "Model's Action: 2\n",
      "Moving left\n",
      "Moving right\n",
      "Episode 1, Total Reward: -19.0, action state: 1\n",
      "Model's Action: 2\n",
      "Moving left\n",
      "Moving left\n",
      "Episode 1, Total Reward: -20.0, action state: 2\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -20.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -20.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -20.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -20.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -20.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Moving left\n",
      "Episode 1, Total Reward: -21.0, action state: 2\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -21.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -21.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -21.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -21.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -21.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -21.0, action state: 0\n",
      "Model's Action: 2\n",
      "Moving left\n",
      "Moving left\n",
      "Episode 1, Total Reward: -22.0, action state: 2\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -22.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -22.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -22.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -22.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -22.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -22.0, action state: 0\n",
      "Model's Action: 2\n",
      "Moving left\n",
      "Moving left\n",
      "Episode 1, Total Reward: -23.0, action state: 2\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -23.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -23.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Moving right\n",
      "Episode 1, Total Reward: -23.0, action state: 1\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -23.0, action state: 0\n",
      "Model's Action: 2\n",
      "Moving left\n",
      "Moving left\n",
      "Episode 1, Total Reward: -24.0, action state: 2\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -24.0, action state: 0\n",
      "Model's Action: 2\n",
      "Moving left\n",
      "Moving up\n",
      "Episode 1, Total Reward: -24.0, action state: 3\n",
      "Model's Action: 2\n",
      "Moving left\n",
      "Moving left\n",
      "Episode 1, Total Reward: -24.0, action state: 2\n",
      "Model's Action: 2\n",
      "Moving left\n",
      "Moving left\n",
      "Episode 1, Total Reward: -24.0, action state: 2\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -24.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -25.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Moving up\n",
      "Episode 1, Total Reward: -25.0, action state: 3\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -25.0, action state: 0\n",
      "Model's Action: 2\n",
      "Moving left\n",
      "Moving left\n",
      "Episode 1, Total Reward: -25.0, action state: 2\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -25.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -26.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -26.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -26.0, action state: 0\n",
      "Model's Action: 2\n",
      "Moving left\n",
      "Moving up\n",
      "Episode 1, Total Reward: -26.0, action state: 3\n",
      "Model's Action: 2\n",
      "Moving left\n",
      "Moving left\n",
      "Episode 1, Total Reward: -26.0, action state: 2\n",
      "Model's Action: 2\n",
      "Moving left\n",
      "Moving left\n",
      "Episode 1, Total Reward: -26.0, action state: 2\n",
      "Model's Action: 2\n",
      "Moving left\n",
      "Moving left\n",
      "Episode 1, Total Reward: -26.0, action state: 2\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -26.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Moving right\n",
      "Episode 1, Total Reward: -27.0, action state: 1\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -27.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Moving up\n",
      "Episode 1, Total Reward: -27.0, action state: 3\n",
      "Model's Action: 2\n",
      "Moving left\n",
      "Moving left\n",
      "Episode 1, Total Reward: -27.0, action state: 2\n",
      "Model's Action: 2\n",
      "Moving left\n",
      "Moving up\n",
      "Episode 1, Total Reward: -27.0, action state: 3\n",
      "Model's Action: 2\n",
      "Moving left\n",
      "Moving left\n",
      "Episode 1, Total Reward: -27.0, action state: 2\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -27.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -28.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -28.0, action state: 0\n",
      "Model's Action: 2\n",
      "Moving left\n",
      "Moving left\n",
      "Episode 1, Total Reward: -29.0, action state: 2\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -29.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -29.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -29.0, action state: 0\n",
      "Model's Action: 2\n",
      "Moving left\n",
      "Moving left\n",
      "Episode 1, Total Reward: -30.0, action state: 2\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -30.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -30.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -30.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -30.0, action state: 0\n",
      "Model's Action: 2\n",
      "Moving left\n",
      "Moving left\n",
      "Episode 1, Total Reward: -31.0, action state: 2\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -31.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -31.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Moving right\n",
      "Episode 1, Total Reward: -31.0, action state: 1\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -31.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -31.0, action state: 0\n",
      "Model's Action: 2\n",
      "Moving left\n",
      "Moving left\n",
      "Episode 1, Total Reward: -32.0, action state: 2\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -32.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -32.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -32.0, action state: 0\n",
      "Model's Action: 2\n",
      "Moving left\n",
      "Moving left\n",
      "Episode 1, Total Reward: -33.0, action state: 2\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -33.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -33.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -33.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Moving up\n",
      "Episode 1, Total Reward: -33.0, action state: 3\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -33.0, action state: 0\n",
      "Model's Action: 2\n",
      "Moving left\n",
      "Moving right\n",
      "Episode 1, Total Reward: -33.0, action state: 1\n",
      "Model's Action: 2\n",
      "Moving left\n",
      "Moving left\n",
      "Episode 1, Total Reward: -34.0, action state: 2\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -34.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -34.0, action state: 0\n",
      "Model's Action: 2\n",
      "Moving left\n",
      "Moving left\n",
      "Episode 1, Total Reward: -35.0, action state: 2\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Moving right\n",
      "Episode 1, Total Reward: -35.0, action state: 1\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -35.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -35.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -35.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -35.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -35.0, action state: 0\n",
      "Model's Action: 2\n",
      "Moving left\n",
      "Moving left\n",
      "Episode 1, Total Reward: -36.0, action state: 2\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -36.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -36.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -36.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Moving up\n",
      "Episode 1, Total Reward: -36.0, action state: 3\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -36.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -37.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -37.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -37.0, action state: 0\n",
      "Model's Action: 2\n",
      "Moving left\n",
      "Moving left\n",
      "Episode 1, Total Reward: -38.0, action state: 2\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -38.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Moving up\n",
      "Episode 1, Total Reward: -38.0, action state: 3\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -38.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -39.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -39.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -39.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -39.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -39.0, action state: 0\n",
      "Model's Action: 2\n",
      "Moving left\n",
      "Moving left\n",
      "Episode 1, Total Reward: -40.0, action state: 2\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -40.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -40.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -40.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Moving right\n",
      "Episode 1, Total Reward: -40.0, action state: 1\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -40.0, action state: 0\n",
      "Model's Action: 2\n",
      "Moving left\n",
      "Moving left\n",
      "Episode 1, Total Reward: -41.0, action state: 2\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -41.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -41.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -41.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Moving up\n",
      "Episode 1, Total Reward: -41.0, action state: 3\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -41.0, action state: 0\n",
      "Model's Action: 2\n",
      "Moving left\n",
      "Moving up\n",
      "Episode 1, Total Reward: -42.0, action state: 3\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -42.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -42.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -42.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -42.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -42.0, action state: 0\n",
      "Model's Action: 2\n",
      "Moving left\n",
      "Moving left\n",
      "Episode 1, Total Reward: -43.0, action state: 2\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Moving up\n",
      "Episode 1, Total Reward: -43.0, action state: 3\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -43.0, action state: 0\n",
      "Model's Action: 2\n",
      "Moving left\n",
      "Moving left\n",
      "Episode 1, Total Reward: -43.0, action state: 2\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -43.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -44.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -44.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -44.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -44.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -44.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -44.0, action state: 0\n",
      "Model's Action: 0\n",
      "Invalid direction\n",
      "Invalid direction\n",
      "Episode 1, Total Reward: -44.0, action state: 0\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "display Surface quit",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m gui \u001b[38;5;241m=\u001b[39m EnvironmentGUIVersion(model)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mgui\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 204\u001b[0m, in \u001b[0;36mEnvironmentGUIVersion.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpisode \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepisode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Total Reward: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_reward\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, action state: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maction\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    203\u001b[0m \u001b[38;5;66;03m# Update Pygame screen\u001b[39;00m\n\u001b[1;32m--> 204\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscreen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfill\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdraw_snake()\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdraw_pebble()\n",
      "\u001b[1;31merror\u001b[0m: display Surface quit"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "gui = EnvironmentGUIVersion(model)\n",
    "gui.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ee5830",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
