{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Christopher Morales\n",
    "## EE 5830 - Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.10.12)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "# Importing Libraries\n",
    "import pygame\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enabling GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants for the DQN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Screen Resolution\n",
    "WIDTH, HEIGHT = 600, 400\n",
    "\n",
    "# How many grids to have (x by y)\n",
    "GRID_SIZE = 20\n",
    "\n",
    "# Frames Per Second\n",
    "FPS = 60\n",
    "\n",
    "# The size of the replay memory used in experience replay (to store and sample past experience)\n",
    "REPLAY_MEMORY_SIZE = 10000\n",
    "\n",
    "# The number of samples (transitions) randomly sampled from the replay memory\n",
    "BATCH_SIZE = 1000\n",
    "\n",
    "# Lower gamma makes the agent focus on immediate rewards where higher gamma considered future rewards more\n",
    "GAMMA = 0.95\n",
    "\n",
    "# Higher the value then explore (exploration trade off parameter)\n",
    "EPSILON_START = 1.0\n",
    "\n",
    "# Sets the minimum value that epsilon can reach (exploration)\n",
    "EPSILON_END = 0.01\n",
    "\n",
    "# To allow the agent to tansistion from exploration to exploitation as it learns ()\n",
    "EPSILON_DECAY = 0.99\n",
    "\n",
    "# Number of episodes (training time essentially)\n",
    "MAX_EPISODE_VALUE = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Transistion state (how the model can learn from past and current)\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Snake Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Snake:\n",
    "    def __init__(self):\n",
    "        # Set initial position in the grid\n",
    "        self.position = [GRID_SIZE * 2, GRID_SIZE * 2] \n",
    "        \n",
    "        # Initialize the snake's body with three segments, including the initial position\n",
    "        self.body = [list(self.position), [self.position[0] - GRID_SIZE, self.position[1]], [self.position[0] - 2 * GRID_SIZE, self.position[1]]]\n",
    "        \n",
    "        # Set the initial direction of the snake to 'RIGHT'\n",
    "        self.direction = 'RIGHT'\n",
    "        \n",
    "        # Set the initial change direction to the current direction\n",
    "        self.change_to = self.direction\n",
    "\n",
    "    def change_direction(self, new_direction):\n",
    "        \"\"\"\n",
    "        Gets the model action input to determine what direction the snake should go\n",
    "\n",
    "        :param new_direction: an integer value that will be represented as UP, DOWN, RIGHT, or LEFT\n",
    "        return: N/A\n",
    "        \"\"\"\n",
    "        # Map the model's predicted action to the expected directions using modulo\n",
    "        num_directions = 4\n",
    "        new_direction = new_direction % num_directions\n",
    "\n",
    "        # Checks if the new_direction is an integer\n",
    "        if isinstance(new_direction, int):\n",
    "            # If the value is 0 then\n",
    "            if new_direction == 0:\n",
    "                # Move RIGHT\n",
    "                self.direction = 'RIGHT'\n",
    "                \n",
    "                # Verifying the condition input\n",
    "                print(\"Moving right\")\n",
    "            \n",
    "            # If the value is 1 then\n",
    "            elif new_direction == 1:\n",
    "                # Move LEFT\n",
    "                self.direction = 'LEFT'\n",
    "\n",
    "                # Verifying the condition input\n",
    "                print(\"Moving left\")\n",
    "            \n",
    "            # If the value is 2 then\n",
    "            elif new_direction == 2:\n",
    "                # Move UP\n",
    "                self.direction = 'UP'\n",
    "                \n",
    "                # Verifying the condition input\n",
    "                print(\"Moving up\")\n",
    "            \n",
    "            # If the value is 3 then\n",
    "            elif new_direction == 3:\n",
    "                # Move DOWN\n",
    "                self.direction = 'DOWN'\n",
    "                \n",
    "                # Verifying the condition input\n",
    "                print(\"Moving down\")\n",
    "\n",
    "            # If the value is something else\n",
    "            else:\n",
    "                # Verifying the condition input\n",
    "                print(\"Invalid direction\")\n",
    "        \n",
    "        # If the new_direction is a different data type somehow\n",
    "        else:\n",
    "            print(\"Invalid direction format. Expected integer.\")\n",
    "\n",
    "\n",
    "    def move(self, pebble):\n",
    "        \"\"\"\n",
    "        Move the snake in the current direction, update its position and body.\n",
    "\n",
    "        :param pebble: Pebble object representing the food for the snake\n",
    "        :return: True if the snake ate the pebble and False otherwise\n",
    "        \"\"\"\n",
    "        # Move the snake to the right\n",
    "        if self.direction == 'RIGHT':\n",
    "            # Update the x-coordinate of the snake's position\n",
    "            self.position[0] += GRID_SIZE\n",
    "        \n",
    "        # Move the snake to the left\n",
    "        elif self.direction == 'LEFT':\n",
    "            # Update the x-coordinate of the snake's position\n",
    "            self.position[0] -= GRID_SIZE\n",
    "        \n",
    "        # Move the snake upwards\n",
    "        elif self.direction == 'UP':\n",
    "            # Update the y-coordinate of the snake's position\n",
    "            self.position[1] -= GRID_SIZE\n",
    "        \n",
    "        # Move the snake downwards\n",
    "        elif self.direction == 'DOWN':\n",
    "            # Update the y-coordinate of the snake's position\n",
    "            self.position[1] += GRID_SIZE\n",
    "\n",
    "        # Update the body positions\n",
    "        self.body.insert(0, list(self.position))\n",
    "        \n",
    "        # Check if the snake's position coincides with the pebble's position\n",
    "        if self.position == pebble.position:\n",
    "            # Snake ate the pebble, grow the body\n",
    "            return True  \n",
    "        \n",
    "        # If the snake did not eat the pebble\n",
    "        else:\n",
    "            # Remove the last segment\n",
    "            self.body.pop()\n",
    "\n",
    "            # Snake did not eat the pebble\n",
    "            return False  \n",
    "\n",
    "    def check_collision(self):\n",
    "        \"\"\"\n",
    "        Check if the snake has collided with the border or itself.\n",
    "\n",
    "        :return: True if collision occurred, False otherwise\n",
    "        \"\"\"\n",
    "        # Check if the snake's x-coordinate is outside the game window\n",
    "        if (\n",
    "            self.position[0] >= WIDTH\n",
    "            or self.position[0] < 0\n",
    "            or self.position[1] >= HEIGHT\n",
    "            or self.position[1] < 0\n",
    "        ):\n",
    "            # Snake collided with the border\n",
    "            return True  \n",
    "        \n",
    "        # Check if the snake collided with itself\n",
    "        for segment in self.body[1:]:\n",
    "            # Compare each body segment with the snake's current position\n",
    "            if segment == self.position:\n",
    "                # Snake collided with itself\n",
    "                return True  \n",
    "        # If no collision occurred\n",
    "        return False\n",
    "\n",
    "    def get_head_position(self):\n",
    "        \"\"\"\n",
    "        Get the position of the snake's head.\n",
    "\n",
    "        :return: List representing the x and y coordinates of the head position\n",
    "        \"\"\"\n",
    "        # Return the current position of the snake's head\n",
    "        return self.position\n",
    "\n",
    "    def get_body_positions(self):\n",
    "        \"\"\"\n",
    "        Get the positions of all segments in the snake's body.\n",
    "\n",
    "        :return: List of lists representing x and y coordinates of each body segment\n",
    "        \"\"\"\n",
    "        # Return the positions of all segments in the snake's body\n",
    "        return self.body\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pebble Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pebble:\n",
    "    def __init__(self):\n",
    "        # Initialize the pebble's position randomly within the grid\n",
    "        self.position = [random.randrange(1, (WIDTH//GRID_SIZE)) * GRID_SIZE,\n",
    "                         random.randrange(1, (HEIGHT//GRID_SIZE)) * GRID_SIZE]\n",
    "\n",
    "    def respawn(self):\n",
    "        \"\"\"\n",
    "        Respawn the pebble at a new random position within the grid.\n",
    "        \n",
    "        return: None\n",
    "        \"\"\"\n",
    "        # Set the pebble's position to a new random position within the grid\n",
    "        self.position = [random.randrange(1, (WIDTH//GRID_SIZE)) * GRID_SIZE,\n",
    "                         random.randrange(1, (HEIGHT//GRID_SIZE)) * GRID_SIZE]\n",
    "\n",
    "    def get_position(self):\n",
    "        \"\"\"\n",
    "        Get the current position of the pebble.\n",
    "        \n",
    "        return: List representing the pebble's position [x, y]\n",
    "        \"\"\"\n",
    "        # Return the current position of the pebble\n",
    "        return self.position\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN Model Class OLD (Conv2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDQNModelOld(nn.Module):\n",
    "    def __init__(self, input_channels, input_height, input_width, n_actions):\n",
    "        # Initialize the DQN model as a subclass of nn.Module\n",
    "        super(MyDQNModelOld, self).__init__()\n",
    "\n",
    "        # Define the first convolutional layer with input channels 1, output channels 32, and kernel size 3 [Conv2D needs to be in 4D]\n",
    "        self.convolution_one = nn.Conv2d(in_channels=input_channels, out_channels=32, kernel_size=3)\n",
    "\n",
    "        # Define the second convolutional layer with input channels 32, output channels 64, and kernel size 3\n",
    "        self.convolution_two = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3)\n",
    "\n",
    "        # Define the first fully connected layer with input size 12 and output size 64\n",
    "        self.function_one = nn.Linear(12, 64)\n",
    "\n",
    "        # Define the second fully connected layer with input size 64 and output size 64\n",
    "        self.function_two = nn.Linear(64, 128)\n",
    "\n",
    "        # Define the third fully connected layer with input size 64 and output size n_actions\n",
    "        # (representing the possible actions the model can take)\n",
    "        self.function_three = nn.Linear(128, n_actions)\n",
    "\n",
    "    def _calculate_conv_output_size(self, h, w):\n",
    "        with torch.no_grad():\n",
    "            # Function to calculate the height and width after applying Conv2d layers\n",
    "            conv1 = self.convolution_one(torch.zeros(1, 1, h, w))\n",
    "            conv2 = self.convolution_two(conv1)\n",
    "        return conv2.size(2), conv2.size(3)  #conv2.view(1, -1).size(1) # conv2.size(2), conv2.size(3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Print the shape of the input x\n",
    "        print(f\"Shape of input x: {x.size()}\")\n",
    "        \n",
    "        # Apply ReLU activation to the output of the first convolutional layer\n",
    "        x = F.relu(self.convolution_one(x))\n",
    "\n",
    "        # Print the shape after the first convolutional layer\n",
    "        print(f\"Shape after first convolutional layer: {x.size()}\")\n",
    "\n",
    "        # Apply ReLU activation to the output of the second convolutional layer\n",
    "        x = F.relu(self.convolution_two(x))\n",
    "\n",
    "        # Print the shape after the second convolutional layer\n",
    "        print(f\"Shape after second convolutional layer: {x.size()}\")\n",
    "\n",
    "        # Flatten the output of Conv2d layers for fully connected layers\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # Print the size after flattening\n",
    "        print(f\"Size after flattening: {x.size()}\")\n",
    "\n",
    "        # Apply ReLU activation to the output of the first fully connected layer\n",
    "        x = torch.relu(self.function_one(x))\n",
    "\n",
    "        # Print the size after the first fully connected layer\n",
    "        print(f\"Size after first fully connected layer: {x.size()}\")\n",
    "\n",
    "        # Apply ReLU activation to the output of the second fully connected layer\n",
    "        x = torch.relu(self.function_two(x))\n",
    "\n",
    "        # Print the size after the second fully connected layer\n",
    "        print(f\"Size after second fully connected layer: {x.size()}\")\n",
    "\n",
    "        # Output the Q-values for each possible action without activation\n",
    "        return self.function_three(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDQNModel(nn.Module):\n",
    "    def __init__(self, input_channels, input_height, input_width, n_actions):\n",
    "        super(MyDQNModel, self).__init__()\n",
    "        self.convolution_one = nn.Conv2d(in_channels=input_channels, out_channels=32, kernel_size=3)\n",
    "        self.convolution_two = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3)\n",
    "        self.function_one = nn.Linear(12, 64)\n",
    "        self.function_two = nn.Linear(64, 128)\n",
    "        self.function_three = nn.Linear(128, n_actions)\n",
    "\n",
    "    def _calculate_conv_output_size(self, h, w, kernel_size, stride, padding):\n",
    "        return ((h - kernel_size + 2 * padding) // stride) + 1, ((w - kernel_size + 2 * padding) // stride) + 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.convolution_one(x))\n",
    "        x = F.relu(self.convolution_two(x))\n",
    "        x = F.relu(self.function_one(x))\n",
    "        x = F.relu(self.function_two(x))\n",
    "        x = self.function_three(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Memory Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    def __init__(self):\n",
    "        # Initialize an empty list to store replay memory transitions\n",
    "        self.memory = []\n",
    "\n",
    "        # Initialize the position in the memory buffer\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"\n",
    "        Add a transition to the replay memory.\n",
    "\n",
    "        :param *args: A tuple representing a transition (state, action, next_state, reward).\n",
    "        return: None\n",
    "        \"\"\"\n",
    "        # If the memory size is less than the maximum allowed size, append None to the memory list\n",
    "        if len(self.memory) < REPLAY_MEMORY_SIZE:\n",
    "            self.memory.append(None)\n",
    "\n",
    "        # Store the transition at the current position in the memory buffer\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "\n",
    "        # Update the position in the memory buffer using modulo to create a circular buffer\n",
    "        self.position = (self.position + 1) % REPLAY_MEMORY_SIZE\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"\n",
    "        Randomly sample a batch of transitions from the replay memory.\n",
    "\n",
    "        :param batch_size: The number of transitions to sample.\n",
    "        return: A list of sampled transitions.\n",
    "        \"\"\"\n",
    "        # Use random.sample to randomly select a batch of transitions from the memory\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the current size of the replay memory.\n",
    "\n",
    "        return: The number of stored transitions in the replay memory.\n",
    "        \"\"\"\n",
    "        # Return the length of the memory list\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Snake Population Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SnakePopulationAgent:\n",
    "    def __init__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Game Environment Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GameEnvironment:\n",
    "    def __init__(self):\n",
    "        # Initialize the snake object\n",
    "        self.snake = Snake()\n",
    "\n",
    "        # Initialize the pebble object\n",
    "        self.pebble = Pebble()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the game environment by creating a new snake and respawning the pebble.\n",
    "\n",
    "        return: The initial state of the game.\n",
    "        \"\"\"\n",
    "        # Create a new snake\n",
    "        self.snake = Snake()\n",
    "\n",
    "        # Respawn the pebble\n",
    "        self.pebble.respawn()\n",
    "\n",
    "        # Return the initial state of the game\n",
    "        return self.get_state()\n",
    "\n",
    "    def get_state(self):\n",
    "        \"\"\"\n",
    "        Get the current state of the game.\n",
    "\n",
    "        return: A tensor representing the current state of the game.\n",
    "        \"\"\"\n",
    "        # Check if the snake has collided, return a consistent representation for the terminal state\n",
    "        if self.snake.check_collision():\n",
    "            return torch.zeros((1, 1, HEIGHT, WIDTH), dtype=torch.float32)\n",
    "\n",
    "        else:\n",
    "            # Extract relevant information about the state\n",
    "            state = [\n",
    "                self.snake.position[0] / WIDTH,\n",
    "                self.snake.position[1] / HEIGHT,\n",
    "                self.pebble.position[0] / WIDTH,\n",
    "                self.pebble.position[1] / HEIGHT,\n",
    "            ]\n",
    "\n",
    "            # Include body segments in the state representation\n",
    "            for segment in self.snake.body:\n",
    "                state.extend([segment[0] / WIDTH, segment[1] / HEIGHT])\n",
    "\n",
    "            # Pad the state with zeros if needed to match the expected input size\n",
    "            while len(state) < 12:\n",
    "                state.append(0.0)\n",
    "\n",
    "            # Convert the state to a tensor and reshape to [batch_size, channels, height, width]\n",
    "            state = torch.tensor(state, dtype=torch.float32).view(1, 1, 1, 12)\n",
    "\n",
    "            # Return the state as a tensor\n",
    "            return state\n",
    "\n",
    "    # def get_state(self):\n",
    "    #     \"\"\"\n",
    "    #     Get the current state of the game.\n",
    "\n",
    "    #     return: A tensor representing the current state of the game.\n",
    "    #     \"\"\"\n",
    "    #     # Check if the snake has collided, return a consistent representation for the terminal state\n",
    "    #     if self.snake.check_collision():\n",
    "    #         return torch.zeros((1, 1, 1, 12), dtype=torch.float32)\n",
    "\n",
    "    #     else:\n",
    "    #         # Extract relevant information about the state\n",
    "    #         state = [\n",
    "    #             self.snake.position[0] / WIDTH,\n",
    "    #             self.snake.position[1] / HEIGHT,\n",
    "    #             self.pebble.position[0] / WIDTH,\n",
    "    #             self.pebble.position[1] / HEIGHT,\n",
    "    #         ]\n",
    "\n",
    "    #         # Include body segments in the state representation\n",
    "    #         for segment in self.snake.body:\n",
    "    #             state.extend([segment[0] / WIDTH, segment[1] / HEIGHT])\n",
    "\n",
    "    #         # Pad the state with zeros if needed to match the expected input size\n",
    "    #         while len(state) < 12:\n",
    "    #             state.append(0.0)\n",
    "\n",
    "    #         # Convert the state to a tensor and reshape to [batch_size, channels, height, width]\n",
    "    #         state = torch.tensor(state, dtype=torch.float32).view(1, 1, 1, -1)\n",
    "\n",
    "    #         # Return the state as a tensor\n",
    "    #         return state\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Take a step in the environment based on the given action.\n",
    "\n",
    "        :param action: The action to be taken by the snake.\n",
    "        return: The next state and the reward obtained from the step.\n",
    "        \"\"\"\n",
    "        # Change the snake's direction based on the action\n",
    "        self.snake.change_direction(action)\n",
    "\n",
    "        # Move the snake and check if it ate the pebble\n",
    "        pebble_eaten = self.snake.move(self.pebble)\n",
    "\n",
    "        # Handle rewards based on the game state\n",
    "        if pebble_eaten:\n",
    "            reward = 1.0\n",
    "            self.pebble.respawn()\n",
    "\n",
    "        elif self.snake.check_collision():\n",
    "            reward = -1.0\n",
    "            self.reset()\n",
    "        \n",
    "        else:\n",
    "            reward = 0.0\n",
    "\n",
    "        # Get the next state after the step\n",
    "        next_state = self.get_state()\n",
    "\n",
    "        # Return the next state and the obtained reward\n",
    "        return next_state, reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment GUI Version Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvironmentGUIVersion:\n",
    "    def __init__(self, model=None, n_observations=12):\n",
    "        # Initialize Pygame\n",
    "        pygame.init()\n",
    "        \n",
    "        # Pygame clock for controlling frame rate\n",
    "        self.clock = pygame.time.Clock()\n",
    "        \n",
    "        # Initialize the game environment\n",
    "        self.game_env = GameEnvironment()\n",
    "        \n",
    "        # Set up the Pygame screen\n",
    "        self.screen = pygame.display.set_mode((WIDTH, HEIGHT))\n",
    "        \n",
    "        # Set up the font for displaying the score\n",
    "        self.font = pygame.font.SysFont(None, 25)\n",
    "        \n",
    "        # Number of possible actions (UP, DOWN, LEFT, RIGHT)\n",
    "        self.n_actions = 4\n",
    "        \n",
    "        # Use the provided model or create a new one\n",
    "        self.model = model if isinstance(model, MyDQNModel) else MyDQNModel(\n",
    "            input_channels=1,           # Adjust based on your input channels\n",
    "            input_height=HEIGHT,        # Adjust based on your input height\n",
    "            input_width=WIDTH,          # Adjust based on your input width\n",
    "            n_actions=self.n_actions    # Number of actions (UP, DOWN, LEFT, RIGHT)\n",
    "        )\n",
    "        \n",
    "        # Exploration-exploitation trade-off parameter\n",
    "        self.epsilon = EPSILON_START\n",
    "        \n",
    "        # Initialize optimizer if a model is provided\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001) if self.model is not None else None\n",
    "        \n",
    "        # Loss functions for training the model\n",
    "        self.mse_loss = nn.MSELoss() if self.model is not None else None\n",
    "        self.mae_loss = nn.L1Loss() if self.model is not None else None\n",
    "        self.smooth_L1_loss = nn.SmoothL1Loss() if self.model is not None else None\n",
    "        self.huber_loss = nn.HuberLoss(reduction='mean', delta=1.0) if self.model is not None else None\n",
    "        \n",
    "        # Replay memory for experience replay\n",
    "        self.memory = ReplayMemory()\n",
    "        \n",
    "\n",
    "    def handle_events(self):\n",
    "        \"\"\"\n",
    "        Handle Pygame events, e.g., window close.\n",
    "        \"\"\"\n",
    "        # Iterate through all Pygame events\n",
    "        for event in pygame.event.get():\n",
    "            # Check if the user closed the window\n",
    "            if event.type == pygame.QUIT:\n",
    "                # Quit Pygame and exit the program\n",
    "                pygame.quit()\n",
    "                quit()\n",
    "\n",
    "        # Use the lambda function to get the state for the model\n",
    "        state = (lambda env: env.get_state())(self.game_env)\n",
    "        \n",
    "        # Convert the state to a tensor and add a batch dimension\n",
    "        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "        # Verifying the state is the correction dimension\n",
    "        print(f\"State shape before model: {state.shape}\")\n",
    "\n",
    "        # Get the action predicted by the model\n",
    "        with torch.no_grad():\n",
    "            action = self.model(state).argmax().item()\n",
    "\n",
    "        # Print the action predicted by the model\n",
    "        print(f\"Model's Action: {action}\")\n",
    "        \n",
    "        # Change the snake's direction based on the predicted action\n",
    "        self.game_env.snake.change_direction(action)\n",
    "    \n",
    "\n",
    "    def draw_snake(self):\n",
    "        \"\"\"\n",
    "        Draw the snake on the Pygame screen.\n",
    "        \"\"\"\n",
    "        # Iterate through all snake body segments\n",
    "        for segment in self.game_env.snake.get_body_positions():\n",
    "            # Draw a green rectangle for each snake segment\n",
    "            pygame.draw.rect(self.screen, (0, 255, 0), pygame.Rect(segment[0], segment[1], GRID_SIZE, GRID_SIZE))\n",
    "\n",
    "    def draw_pebble(self):\n",
    "        \"\"\"\n",
    "        Draw the pebble on the Pygame screen.\n",
    "        \"\"\"\n",
    "        # Get the pebble's position\n",
    "        position = self.game_env.pebble.get_position()\n",
    "        \n",
    "        # Draw a red rectangle for the pebble\n",
    "        pygame.draw.rect(self.screen, (255, 0, 0), pygame.Rect(position[0], position[1], GRID_SIZE, GRID_SIZE))\n",
    "\n",
    "    def draw_score(self, score):\n",
    "        \"\"\"\n",
    "        Draw the current score on the Pygame screen.\n",
    "        \n",
    "        :param score: The current score to display.\n",
    "        \"\"\"\n",
    "        # Render the score text\n",
    "        score_text = self.font.render(f'Score: {score}', True, (255, 255, 255))\n",
    "        \n",
    "        # Blit the score text onto the screen\n",
    "        self.screen.blit(score_text, (10, 10))\n",
    "\n",
    "    def draw_grid(self):\n",
    "        \"\"\"\n",
    "        Draw the grid lines on the Pygame screen.\n",
    "        \"\"\"\n",
    "        # Draw vertical grid lines\n",
    "        for x in range(0, WIDTH, GRID_SIZE):\n",
    "            pygame.draw.line(self.screen, (50, 50, 50), (x, 0), (x, HEIGHT))\n",
    "        \n",
    "        # Draw horizontal grid lines\n",
    "        for y in range(0, HEIGHT, GRID_SIZE):\n",
    "            pygame.draw.line(self.screen, (50, 50, 50), (0, y), (WIDTH, y))\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Main loop for running the environment.\n",
    "        \"\"\"\n",
    "        # Flag indicating whether the game is running\n",
    "        running = True\n",
    "\n",
    "        # Main loop iterating over episodes\n",
    "        for episode in range(1, MAX_EPISODE_VALUE):\n",
    "            # Reset the game environment for a new episode\n",
    "            state = self.game_env.reset()\n",
    "            \n",
    "            # Total reward accumulated during the episode\n",
    "            total_reward = 0\n",
    "\n",
    "            # Episode loop\n",
    "            while True:\n",
    "                # Handle events outside the main loop\n",
    "                self.handle_events()  \n",
    "\n",
    "                # Exploration-exploitation strategy\n",
    "                if random.random() < self.epsilon:\n",
    "                    # Explore: Choose a random action\n",
    "                    action = random.randint(0, 3)\n",
    "                \n",
    "                else:\n",
    "                    # Exploit: Choose the action with the highest Q-value\n",
    "                    with torch.no_grad():\n",
    "                        action = self.model(state).argmax().item()\n",
    "\n",
    "                # Take a step in the environment\n",
    "                next_state, reward = self.game_env.step(action)\n",
    "                \n",
    "                # Update total reward\n",
    "                total_reward += reward\n",
    "\n",
    "                # Store the experience in the replay memory\n",
    "                self.memory.push(state, action, next_state, reward)\n",
    "                \n",
    "                # Update the current state\n",
    "                state = next_state\n",
    "\n",
    "                # Train the model if enough experiences are stored in the memory\n",
    "                if len(self.memory) > BATCH_SIZE:\n",
    "                    # Sample a batch of experiences from the replay memory\n",
    "                    batch = Transition(*zip(*self.memory.sample(BATCH_SIZE)))\n",
    "                    \n",
    "                    # Create a mask indicating non-final next states\n",
    "                    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)),\n",
    "                                                  dtype=torch.bool)\n",
    "                    \n",
    "                    # Determine the maximum length among next states\n",
    "                    max_len = max(s.shape[1] if s is not None else 0 for s in batch.next_state)\n",
    "\n",
    "                    # Pad non-final next states to have the same length\n",
    "                    non_final_next_states = torch.cat([\n",
    "                        torch.nn.functional.pad(s, (0, max_len - s.shape[1])) if s is not None else torch.zeros(1, max_len)\n",
    "                        for s in batch.next_state\n",
    "                    ])\n",
    "\n",
    "                    # Correct input size\n",
    "                    input_size = len(self.game_env.get_state()[0])\n",
    "                    \n",
    "                    # Create a new model with the correct input size\n",
    "                    self.model = MyDQNModel(input_size, self.n_actions)\n",
    "                    \n",
    "                    # Convert the batch data to tensors\n",
    "                    state_batch = torch.cat(batch.state)\n",
    "                    action_batch = torch.tensor(batch.action, dtype=torch.long).view(-1, 1)\n",
    "                    reward_batch = torch.tensor(batch.reward, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "                    # Compute Q-values for the current state and selected actions\n",
    "                    Q_current = self.model(state_batch).gather(1, action_batch)\n",
    "                    \n",
    "                    # Initialize tensor for Q-values of next states\n",
    "                    Q_next = torch.zeros(BATCH_SIZE)\n",
    "\n",
    "                    # Update Q-values for non-final next states\n",
    "                    Q_next[non_final_mask] = self.model(non_final_next_states).max(1)[0].detach()\n",
    "                    \n",
    "                    # Compute Q-target values for the Bellman equation\n",
    "                    Q_target = reward_batch + (GAMMA * Q_next)\n",
    "\n",
    "                    # Compute the Huber loss between current and target Q-values\n",
    "                    loss = self.huber_loss(Q_current, Q_target)\n",
    "                    \n",
    "                    # Zero gradients, perform a backward pass, and update the weights\n",
    "                    self.optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "\n",
    "                # Decay epsilon for exploration-exploitation\n",
    "                self.epsilon = max(EPSILON_END, self.epsilon * EPSILON_DECAY)\n",
    "                print(f\"Episode {episode}, Total Reward: {total_reward}, action state: {action}\")\n",
    "\n",
    "                # Update Pygame screen\n",
    "                self.screen.fill((0, 0, 0))\n",
    "                self.draw_snake()\n",
    "                self.draw_pebble()\n",
    "                self.draw_score(total_reward)\n",
    "                self.draw_grid()\n",
    "                pygame.display.flip()\n",
    "\n",
    "                # Cap the frame rate\n",
    "                self.clock.tick(FPS)\n",
    "\n",
    "                # Exit the loop if the environment is no longer running\n",
    "                if not running:\n",
    "                    pygame.quit()\n",
    "                    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an instance for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the input size based on the state representation in the environment\n",
    "input_size = len(EnvironmentGUIVersion().game_env.get_state()[0])\n",
    "\n",
    "# Assuming 4 possible actions (UP, DOWN, LEFT, RIGHT)\n",
    "n_actions = 4\n",
    "\n",
    "# Assuming your game state is represented as a 1D tensor\n",
    "input_channels = 1\n",
    "\n",
    "# Create an instance of the MyDQNModel class with the calculated input size and number of actions [batch size, channels, height, width]\n",
    "model = MyDQNModel(1, 1, 12, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Cell (Combining everything)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\artorias961\\AppData\\Local\\Temp\\ipykernel_4992\\1869275723.py:61: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State shape before model: torch.Size([1, 1, 1, 1, 12])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [1, 12]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m gui \u001b[38;5;241m=\u001b[39m EnvironmentGUIVersion(model)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mgui\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[12], line 138\u001b[0m, in \u001b[0;36mEnvironmentGUIVersion.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;66;03m# Episode loop\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;66;03m# Handle events outside the main loop\u001b[39;00m\n\u001b[1;32m--> 138\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_events\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[0;32m    140\u001b[0m     \u001b[38;5;66;03m# Exploration-exploitation strategy\u001b[39;00m\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m random\u001b[38;5;241m.\u001b[39mrandom() \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon:\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;66;03m# Explore: Choose a random action\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[12], line 68\u001b[0m, in \u001b[0;36mEnvironmentGUIVersion.handle_events\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# Get the action predicted by the model\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 68\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39margmax()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# Print the action predicted by the model\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms Action: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maction\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\artorias961\\mambaforge\\envs\\ee5830\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[8], line 15\u001b[0m, in \u001b[0;36mMyDQNModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     14\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 15\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvolution_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     16\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvolution_two(x))\n\u001b[0;32m     17\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_one(x))\n",
      "File \u001b[1;32mc:\\Users\\artorias961\\mambaforge\\envs\\ee5830\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\artorias961\\mambaforge\\envs\\ee5830\\lib\\site-packages\\torch\\nn\\modules\\conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\artorias961\\mambaforge\\envs\\ee5830\\lib\\site-packages\\torch\\nn\\modules\\conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [1, 12]"
     ]
    }
   ],
   "source": [
    "gui = EnvironmentGUIVersion(model)\n",
    "gui.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ee5830",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
