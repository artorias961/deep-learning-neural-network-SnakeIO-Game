{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Christopher Morales\n",
    "## EE 5830 - Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.10.12)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "# Importing Libraries\n",
    "import pygame\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants for the DQN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Screen Resolution\n",
    "WIDTH, HEIGHT = 600, 400\n",
    "\n",
    "# How many grids to have (x by x)\n",
    "GRID_SIZE = 20\n",
    "\n",
    "# Frames Per Second\n",
    "FPS = 60\n",
    "\n",
    "# The size of the replay memory used in experience replay (to store and sample past experience)\n",
    "REPLAY_MEMORY_SIZE = 10000\n",
    "\n",
    "# The number of samples (transitions) randomly sampled from the replay memory\n",
    "BATCH_SIZE = 1000\n",
    "\n",
    "# Lower gamma makes the agent focus on immediate rewards where higher gamma considered future rewards more\n",
    "GAMMA = 0.95\n",
    "\n",
    "# Higher the value then explore (exploration trade off parameter)\n",
    "EPSILON_START = 1.0\n",
    "\n",
    "# Sets the minimum value that epsilon can reach (exploration)\n",
    "EPSILON_END = 0.01\n",
    "\n",
    "# To allow the agent to tansistion from exploration to exploitation as it learns ()\n",
    "EPSILON_DECAY = 0.99\n",
    "\n",
    "# Number of episodes (training time essentially)\n",
    "MAX_EPISODE_VALUE = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Transistion state (how the model can learn from past and current)\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Snake Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Snake:\n",
    "    def __init__(self):\n",
    "        # Set initial position in the grid\n",
    "        self.position = [GRID_SIZE * 2, GRID_SIZE * 2] \n",
    "        \n",
    "        # Initialize the snake's body with three segments, including the initial position\n",
    "        self.body = [list(self.position), [self.position[0] - GRID_SIZE, self.position[1]], [self.position[0] - 2 * GRID_SIZE, self.position[1]]]\n",
    "        \n",
    "        # Set the initial direction of the snake to 'RIGHT'\n",
    "        self.direction = 'RIGHT'\n",
    "        \n",
    "        # Set the initial change direction to the current direction\n",
    "        self.change_to = self.direction\n",
    "\n",
    "    def change_direction(self, new_direction):\n",
    "        \"\"\"\n",
    "        Gets the model action input to determine what direction the snake should go\n",
    "\n",
    "        :param new_direction: an integer value that will be represented as UP, DOWN, RIGHT, or LEFT\n",
    "        return: N/A\n",
    "        \"\"\"\n",
    "        # Checks if the new_direction is an integer\n",
    "        if isinstance(new_direction, int):\n",
    "            # If the value is 1 then\n",
    "            if new_direction == 1:\n",
    "                # Move RIGHT\n",
    "                self.direction = 'RIGHT'\n",
    "                \n",
    "                # Verifying the condition input\n",
    "                print(\"Moving right\")\n",
    "            \n",
    "            # If the value is 1 then\n",
    "            elif new_direction == 2:\n",
    "                # Move LEFT\n",
    "                self.direction = 'LEFT'\n",
    "\n",
    "                # Verifying the condition input\n",
    "                print(\"Moving left\")\n",
    "            \n",
    "            # If the value is 1 then\n",
    "            elif new_direction == 3:\n",
    "                # Move UP\n",
    "                self.direction = 'UP'\n",
    "                \n",
    "                # Verifying the condition input\n",
    "                print(\"Moving up\")\n",
    "            \n",
    "            # If the value is 1 then\n",
    "            elif new_direction == 4:\n",
    "                # Move DOWN\n",
    "                self.direction = 'DOWN'\n",
    "                \n",
    "                # Verifying the condition input\n",
    "                print(\"Moving down\")\n",
    "\n",
    "            # If the value is something else\n",
    "            else:\n",
    "                # Verifying the condition input\n",
    "                print(\"Invalid direction\")\n",
    "        \n",
    "        # If the new_direction is a different data type somehow\n",
    "        else:\n",
    "            print(\"Invalid direction format. Expected integer.\")\n",
    "\n",
    "\n",
    "    def move(self, pebble):\n",
    "        \"\"\"\n",
    "        Move the snake in the current direction, update its position and body.\n",
    "\n",
    "        :param pebble: Pebble object representing the food for the snake\n",
    "        :return: True if the snake ate the pebble and False otherwise\n",
    "        \"\"\"\n",
    "        # Move the snake to the right\n",
    "        if self.direction == 'RIGHT':\n",
    "            # Update the x-coordinate of the snake's position\n",
    "            self.position[0] += GRID_SIZE\n",
    "        \n",
    "        # Move the snake to the left\n",
    "        elif self.direction == 'LEFT':\n",
    "            # Update the x-coordinate of the snake's position\n",
    "            self.position[0] -= GRID_SIZE\n",
    "        \n",
    "        # Move the snake upwards\n",
    "        elif self.direction == 'UP':\n",
    "            # Update the y-coordinate of the snake's position\n",
    "            self.position[1] -= GRID_SIZE\n",
    "        \n",
    "        # Move the snake downwards\n",
    "        elif self.direction == 'DOWN':\n",
    "            # Update the y-coordinate of the snake's position\n",
    "            self.position[1] += GRID_SIZE\n",
    "\n",
    "        # Update the body positions\n",
    "        self.body.insert(0, list(self.position))\n",
    "        \n",
    "        # Check if the snake's position coincides with the pebble's position\n",
    "        if self.position == pebble.position:\n",
    "            # Snake ate the pebble, grow the body\n",
    "            return True  \n",
    "        \n",
    "        # If the snake did not eat the pebble\n",
    "        else:\n",
    "            # Remove the last segment\n",
    "            self.body.pop()\n",
    "\n",
    "            # Snake did not eat the pebble\n",
    "            return False  \n",
    "\n",
    "    def check_collision(self):\n",
    "        \"\"\"\n",
    "        Check if the snake has collided with the border or itself.\n",
    "\n",
    "        :return: True if collision occurred, False otherwise\n",
    "        \"\"\"\n",
    "        # Check if the snake's x-coordinate is outside the game window\n",
    "        if (\n",
    "            self.position[0] >= WIDTH\n",
    "            or self.position[0] < 0\n",
    "            or self.position[1] >= HEIGHT\n",
    "            or self.position[1] < 0\n",
    "        ):\n",
    "            # Snake collided with the border\n",
    "            return True  \n",
    "        \n",
    "        # Check if the snake collided with itself\n",
    "        for segment in self.body[1:]:\n",
    "            # Compare each body segment with the snake's current position\n",
    "            if segment == self.position:\n",
    "                # Snake collided with itself\n",
    "                return True  \n",
    "        # If no collision occurred\n",
    "        return False\n",
    "\n",
    "    def get_head_position(self):\n",
    "        \"\"\"\n",
    "        Get the position of the snake's head.\n",
    "\n",
    "        :return: List representing the x and y coordinates of the head position\n",
    "        \"\"\"\n",
    "        # Return the current position of the snake's head\n",
    "        return self.position\n",
    "\n",
    "    def get_body_positions(self):\n",
    "        \"\"\"\n",
    "        Get the positions of all segments in the snake's body.\n",
    "\n",
    "        :return: List of lists representing x and y coordinates of each body segment\n",
    "        \"\"\"\n",
    "        # Return the positions of all segments in the snake's body\n",
    "        return self.body\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pebble Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pebble:\n",
    "    def __init__(self):\n",
    "        # Initialize the pebble's position randomly within the grid\n",
    "        self.position = [random.randrange(1, (WIDTH//GRID_SIZE)) * GRID_SIZE,\n",
    "                         random.randrange(1, (HEIGHT//GRID_SIZE)) * GRID_SIZE]\n",
    "\n",
    "    def respawn(self):\n",
    "        \"\"\"\n",
    "        Respawn the pebble at a new random position within the grid.\n",
    "        \n",
    "        return: None\n",
    "        \"\"\"\n",
    "        # Set the pebble's position to a new random position within the grid\n",
    "        self.position = [random.randrange(1, (WIDTH//GRID_SIZE)) * GRID_SIZE,\n",
    "                         random.randrange(1, (HEIGHT//GRID_SIZE)) * GRID_SIZE]\n",
    "\n",
    "    def get_position(self):\n",
    "        \"\"\"\n",
    "        Get the current position of the pebble.\n",
    "        \n",
    "        return: List representing the pebble's position [x, y]\n",
    "        \"\"\"\n",
    "        # Return the current position of the pebble\n",
    "        return self.position\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDQNModel(nn.Module):\n",
    "    def __init__(self, n_observation, n_actions, n_input_channels, input_image_height, n_output_probs,\n",
    "                 conv_layer_sizes=(32, 64, ), conv_kernel_sizes=(3, 3, ), act_func_maxpool=F.relu,\n",
    "                 dense_layer_sizes=(100, 100, ), act_func_dense=F.relu, dropout=0.5):\n",
    "        super(MyDQNModel, self).__init__()\n",
    "\n",
    "        # Calculate the output size after the convolutional layers\n",
    "        self.conv_out_size = self._calculate_conv_out_size(n_observation)\n",
    "\n",
    "        # Initialize the convolutional network\n",
    "        self.conv_network = nn.ModuleList()\n",
    "\n",
    "        self.conv_network.append(\n",
    "            nn.Conv2d(in_channels=1, out_channels=conv_layer_sizes[0], kernel_size=conv_kernel_sizes[0])\n",
    "        )\n",
    "\n",
    "        self.conv_network.append(nn.MaxPool2d(2))\n",
    "\n",
    "        for i in range(len(conv_layer_sizes) - 1):\n",
    "            self.conv_network.append(\n",
    "                nn.Conv2d(in_channels=conv_layer_sizes[i], out_channels=conv_layer_sizes[i + 1],\n",
    "                          kernel_size=conv_kernel_sizes[i + 1])\n",
    "            )\n",
    "            self.conv_network.append(nn.Dropout(p=dropout))\n",
    "            self.conv_network.append(nn.MaxPool2d(2))\n",
    "\n",
    "        # Calculate the number of inputs into the dense network\n",
    "        self.dense_n_inputs = self._calc_dense_n_inputs(input_image_height, conv_kernel_sizes, conv_layer_sizes)\n",
    "\n",
    "        # Initialize the dense network\n",
    "        self.dense_network = nn.ModuleList()\n",
    "\n",
    "        self.dense_network.append(nn.Linear(self.dense_n_inputs, dense_layer_sizes[0]))\n",
    "\n",
    "        for i in range(len(dense_layer_sizes) - 1):\n",
    "            self.dense_network.append(nn.Linear(dense_layer_sizes[i], dense_layer_sizes[i + 1]))\n",
    "\n",
    "        # Output layer\n",
    "        self.output = nn.Linear(dense_layer_sizes[-1], n_output_probs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Assuming x is a 2D tensor with shape [batch_size, input_size]\n",
    "        x = x.view(-1, 1, 1, x.size(1))\n",
    "\n",
    "        for layer in self.conv_network:\n",
    "            if isinstance(layer, nn.MaxPool2d):\n",
    "                x = F.relu(layer(x))\n",
    "            else:\n",
    "                x = layer(x)\n",
    "\n",
    "        x = x.view(-1, x.size(1) * x.size(2) * x.size(3))\n",
    "\n",
    "        for layer in self.dense_network:\n",
    "            x = F.relu(layer(x))\n",
    "\n",
    "        return F.softmax(self.output(x), dim=-1)\n",
    "\n",
    "    def _calculate_conv_out_size(self, input_size):\n",
    "        # dummy_input = torch.zeros((1, 1, *input_size), dtype=torch.float32)\n",
    "        dummy_input = torch.zeros((1, 1, input_size, 1), dtype=torch.float32)\n",
    "        conv_out = self._convolutional_layers(dummy_input)\n",
    "        return conv_out.view(conv_out.size(0), -1).size(1)\n",
    "\n",
    "    def _convolutional_layers(self, x):\n",
    "        for layer in self.conv_network:\n",
    "            if isinstance(layer, nn.MaxPool2d):\n",
    "                x = F.relu(layer(x))\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def _calc_dense_n_inputs(self, input_image_height, conv_kernel_sizes, conv_layer_sizes):\n",
    "        final_size = input_image_height\n",
    "\n",
    "        for conv_kernel_size in conv_kernel_sizes:\n",
    "            final_size = np.floor(final_size - (conv_kernel_size - 1))\n",
    "            final_size = np.floor((final_size - (2 - 1) - 1) / 2 + 1)\n",
    "\n",
    "        return int(conv_layer_sizes[-1] * np.square(final_size))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Memory Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    def __init__(self):\n",
    "        # Initialize an empty list to store replay memory transitions\n",
    "        self.memory = []\n",
    "\n",
    "        # Initialize the position in the memory buffer\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"\n",
    "        Add a transition to the replay memory.\n",
    "\n",
    "        :param *args: A tuple representing a transition (state, action, next_state, reward).\n",
    "        return: None\n",
    "        \"\"\"\n",
    "        # If the memory size is less than the maximum allowed size, append None to the memory list\n",
    "        if len(self.memory) < REPLAY_MEMORY_SIZE:\n",
    "            self.memory.append(None)\n",
    "\n",
    "        # Store the transition at the current position in the memory buffer\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "\n",
    "        # Update the position in the memory buffer using modulo to create a circular buffer\n",
    "        self.position = (self.position + 1) % REPLAY_MEMORY_SIZE\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"\n",
    "        Randomly sample a batch of transitions from the replay memory.\n",
    "\n",
    "        :param batch_size: The number of transitions to sample.\n",
    "        return: A list of sampled transitions.\n",
    "        \"\"\"\n",
    "        # Use random.sample to randomly select a batch of transitions from the memory\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the current size of the replay memory.\n",
    "\n",
    "        return: The number of stored transitions in the replay memory.\n",
    "        \"\"\"\n",
    "        # Return the length of the memory list\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Snake Population Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SnakePopulationAgent:\n",
    "    def __init__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Game Environment Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GameEnvironment:\n",
    "    def __init__(self):\n",
    "        # Initialize the snake object\n",
    "        self.snake = Snake()\n",
    "\n",
    "        # Initialize the pebble object\n",
    "        self.pebble = Pebble()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the game environment by creating a new snake and respawning the pebble.\n",
    "\n",
    "        return: The initial state of the game.\n",
    "        \"\"\"\n",
    "        # Create a new snake\n",
    "        self.snake = Snake()\n",
    "\n",
    "        # Respawn the pebble\n",
    "        self.pebble.respawn()\n",
    "\n",
    "        # Return the initial state of the game\n",
    "        return self.get_state()\n",
    "\n",
    "    def get_state(self):\n",
    "        \"\"\"\n",
    "        Get the current state of the game.\n",
    "\n",
    "        return: A tensor representing the current state of the game.\n",
    "        \"\"\"\n",
    "        # Check if the snake has collided, return a consistent representation for the terminal state\n",
    "        if self.snake.check_collision():\n",
    "            return torch.zeros((1, 12), dtype=torch.float32)\n",
    "        \n",
    "        else:\n",
    "            # Extract relevant information about the state\n",
    "            state = [\n",
    "                self.snake.position[0] / WIDTH,\n",
    "                self.snake.position[1] / HEIGHT,\n",
    "                self.pebble.position[0] / WIDTH,\n",
    "                self.pebble.position[1] / HEIGHT,\n",
    "            ]\n",
    "\n",
    "            # Include body segments in the state representation\n",
    "            for segment in self.snake.body:\n",
    "                state.extend([segment[0] / WIDTH, segment[1] / HEIGHT])\n",
    "\n",
    "            # Pad the state with zeros if needed to match the expected input size\n",
    "            while len(state) < 12:\n",
    "                state.append(0.0)\n",
    "\n",
    "            # Return the state as a tensor\n",
    "            return torch.tensor(state, dtype=torch.float32).view(1, -1)\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Take a step in the environment based on the given action.\n",
    "\n",
    "        :param action: The action to be taken by the snake.\n",
    "        return: The next state and the reward obtained from the step.\n",
    "        \"\"\"\n",
    "        # Change the snake's direction based on the action\n",
    "        self.snake.change_direction(action)\n",
    "\n",
    "        # Move the snake and check if it ate the pebble\n",
    "        pebble_eaten = self.snake.move(self.pebble)\n",
    "\n",
    "        # Handle rewards based on the game state\n",
    "        if pebble_eaten:\n",
    "            reward = 1.0\n",
    "            self.pebble.respawn()\n",
    "\n",
    "        # When the snake dies\n",
    "        elif self.snake.check_collision():\n",
    "            reward = -1.0\n",
    "            self.reset()\n",
    "        \n",
    "        # Nothing happens (still vibing)\n",
    "        else:\n",
    "            reward = 0.0\n",
    "\n",
    "        # Get the next state after the step\n",
    "        next_state = self.get_state()\n",
    "\n",
    "        # Return the next state and the obtained reward\n",
    "        return next_state, reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment GUI Version Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvironmentGUIVersion:\n",
    "    def __init__(self, model=None, n_observations=12):\n",
    "        # Initialize Pygame\n",
    "        pygame.init()\n",
    "        \n",
    "        # Pygame clock for controlling frame rate\n",
    "        self.clock = pygame.time.Clock()\n",
    "        \n",
    "        # Initialize the game environment\n",
    "        self.game_env = GameEnvironment()\n",
    "        \n",
    "        # Set up the Pygame screen\n",
    "        self.screen = pygame.display.set_mode((WIDTH, HEIGHT))\n",
    "        \n",
    "        # Set up the font for displaying the score\n",
    "        self.font = pygame.font.SysFont(None, 25)\n",
    "        \n",
    "        # Number of possible actions (UP, DOWN, LEFT, RIGHT)\n",
    "        self.n_actions = 4\n",
    "        \n",
    "        # Use the provided model or create a new one\n",
    "        # self.model = model if isinstance(model, MyDQNModel) else MyDQNModel(n_observations, self.n_actions)\n",
    "        self.model = model if isinstance(model, MyDQNModel) else MyDQNModel(\n",
    "                                        n_observations, self.n_actions, n_input_channels=1, input_image_height=1, n_output_probs=self.n_actions)\n",
    "\n",
    "        \n",
    "        # Exploration-exploitation trade-off parameter\n",
    "        self.epsilon = EPSILON_START\n",
    "        \n",
    "        # Initialize optimizer if a model is provided\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001) if self.model is not None else None\n",
    "        \n",
    "        # Loss functions for training the model\n",
    "        self.mse_loss = nn.MSELoss() if self.model is not None else None\n",
    "        self.mae_loss = nn.L1Loss() if self.model is not None else None\n",
    "        self.smooth_L1_loss = nn.SmoothL1Loss() if self.model is not None else None\n",
    "        self.huber_loss = nn.HuberLoss(reduction='mean', delta=1.0) if self.model is not None else None\n",
    "        \n",
    "        # Replay memory for experience replay\n",
    "        self.memory = ReplayMemory()\n",
    "\n",
    "    def handle_events(self):\n",
    "        \"\"\"\n",
    "        Handle Pygame events, e.g., window close.\n",
    "        \"\"\"\n",
    "        # Iterate through all Pygame events\n",
    "        for event in pygame.event.get():\n",
    "            # Check if the user closed the window\n",
    "            if event.type == pygame.QUIT:\n",
    "                # Quit Pygame and exit the program\n",
    "                pygame.quit()\n",
    "                quit()\n",
    "\n",
    "        # Use the lambda function to get the state for the model\n",
    "        state = (lambda env: env.get_state())(self.game_env)\n",
    "\n",
    "        # Get the action predicted by the model\n",
    "        with torch.no_grad():\n",
    "            action = self.model(state).argmax().item()\n",
    "\n",
    "        # Print the action predicted by the model\n",
    "        print(f\"Model's Action: {action}\")\n",
    "        \n",
    "        # Change the snake's direction based on the predicted action\n",
    "        self.game_env.snake.change_direction(action)\n",
    "    \n",
    "\n",
    "    def draw_snake(self):\n",
    "        \"\"\"\n",
    "        Draw the snake on the Pygame screen.\n",
    "        \"\"\"\n",
    "        # Iterate through all snake body segments\n",
    "        for segment in self.game_env.snake.get_body_positions():\n",
    "            # Draw a green rectangle for each snake segment\n",
    "            pygame.draw.rect(self.screen, (0, 255, 0), pygame.Rect(segment[0], segment[1], GRID_SIZE, GRID_SIZE))\n",
    "\n",
    "    def draw_pebble(self):\n",
    "        \"\"\"\n",
    "        Draw the pebble on the Pygame screen.\n",
    "        \"\"\"\n",
    "        # Get the pebble's position\n",
    "        position = self.game_env.pebble.get_position()\n",
    "        \n",
    "        # Draw a red rectangle for the pebble\n",
    "        pygame.draw.rect(self.screen, (255, 0, 0), pygame.Rect(position[0], position[1], GRID_SIZE, GRID_SIZE))\n",
    "\n",
    "    def draw_score(self, score):\n",
    "        \"\"\"\n",
    "        Draw the current score on the Pygame screen.\n",
    "        \n",
    "        :param score: The current score to display.\n",
    "        \"\"\"\n",
    "        # Render the score text\n",
    "        score_text = self.font.render(f'Score: {score}', True, (255, 255, 255))\n",
    "        \n",
    "        # Blit the score text onto the screen\n",
    "        self.screen.blit(score_text, (10, 10))\n",
    "\n",
    "    def draw_grid(self):\n",
    "        \"\"\"\n",
    "        Draw the grid lines on the Pygame screen.\n",
    "        \"\"\"\n",
    "        # Draw vertical grid lines\n",
    "        for x in range(0, WIDTH, GRID_SIZE):\n",
    "            pygame.draw.line(self.screen, (50, 50, 50), (x, 0), (x, HEIGHT))\n",
    "        \n",
    "        # Draw horizontal grid lines\n",
    "        for y in range(0, HEIGHT, GRID_SIZE):\n",
    "            pygame.draw.line(self.screen, (50, 50, 50), (0, y), (WIDTH, y))\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Main loop for running the environment.\n",
    "        \"\"\"\n",
    "        # Flag indicating whether the game is running\n",
    "        running = True\n",
    "\n",
    "        # Main loop iterating over episodes\n",
    "        for episode in range(1, MAX_EPISODE_VALUE):\n",
    "            # Reset the game environment for a new episode\n",
    "            state = self.game_env.reset()\n",
    "            \n",
    "            # Total reward accumulated during the episode\n",
    "            total_reward = 0\n",
    "\n",
    "            # Episode loop\n",
    "            while True:\n",
    "                # Handle events outside the main loop\n",
    "                self.handle_events()  \n",
    "\n",
    "                # Exploration-exploitation strategy\n",
    "                if random.random() < self.epsilon:\n",
    "                    # Explore: Choose a random action\n",
    "                    action = random.randint(0, 3)\n",
    "                \n",
    "                else:\n",
    "                    # Exploit: Choose the action with the highest Q-value\n",
    "                    with torch.no_grad():\n",
    "                        action = self.model(state).argmax().item()\n",
    "\n",
    "                # Take a step in the environment\n",
    "                next_state, reward = self.game_env.step(action)\n",
    "                \n",
    "                # Update total reward\n",
    "                total_reward += reward\n",
    "\n",
    "                # Store the experience in the replay memory\n",
    "                self.memory.push(state, action, next_state, reward)\n",
    "                \n",
    "                # Update the current state\n",
    "                state = next_state\n",
    "\n",
    "                # Train the model if enough experiences are stored in the memory\n",
    "                if len(self.memory) > BATCH_SIZE:\n",
    "                    # Sample a batch of experiences from the replay memory\n",
    "                    batch = Transition(*zip(*self.memory.sample(BATCH_SIZE)))\n",
    "                    \n",
    "                    # Create a mask indicating non-final next states\n",
    "                    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)),\n",
    "                                                  dtype=torch.bool)\n",
    "                    \n",
    "                    # Determine the maximum length among next states\n",
    "                    max_len = max(s.shape[1] if s is not None else 0 for s in batch.next_state)\n",
    "\n",
    "                    # Pad non-final next states to have the same length\n",
    "                    non_final_next_states = torch.cat([\n",
    "                        torch.nn.functional.pad(s, (0, max_len - s.shape[1])) if s is not None else torch.zeros(1, max_len)\n",
    "                        for s in batch.next_state\n",
    "                    ])\n",
    "\n",
    "                    # Correct input size\n",
    "                    input_size = len(self.game_env.get_state()[0])\n",
    "\n",
    "                    # Update the existing model's conv_out_size attribute\n",
    "                    self.model.conv_out_size = self.model._calculate_conv_out_size(input_size)\n",
    "                    \n",
    "                    # Create a new model with the correct input size\n",
    "                    # self.model = MyDQNModel(input_size, self.n_actions)\n",
    "                    self.model = MyDQNModel(self.model.conv_out_size, self.n_actions)\n",
    "                    \n",
    "                    # Convert the batch data to tensors\n",
    "                    state_batch = torch.cat(batch.state)\n",
    "                    action_batch = torch.tensor(batch.action, dtype=torch.long).view(-1, 1)\n",
    "                    reward_batch = torch.tensor(batch.reward, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "                    # Compute Q-values for the current state and selected actions\n",
    "                    Q_current = self.model(state_batch).gather(1, action_batch)\n",
    "                    \n",
    "                    # Initialize tensor for Q-values of next states\n",
    "                    Q_next = torch.zeros(BATCH_SIZE)\n",
    "\n",
    "                    # Update Q-values for non-final next states\n",
    "                    Q_next[non_final_mask] = self.model(non_final_next_states).max(1)[0].detach()\n",
    "                    \n",
    "                    # Compute Q-target values for the Bellman equation\n",
    "                    Q_target = reward_batch + (GAMMA * Q_next)\n",
    "\n",
    "                    # Compute the Huber loss between current and target Q-values\n",
    "                    loss = self.huber_loss(Q_current, Q_target)\n",
    "                    \n",
    "                    # Zero gradients, perform a backward pass, and update the weights\n",
    "                    self.optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "\n",
    "                # Decay epsilon for exploration-exploitation\n",
    "                self.epsilon = max(EPSILON_END, self.epsilon * EPSILON_DECAY)\n",
    "                print(f\"Episode {episode}, Total Reward: {total_reward}, action state: {action}\")\n",
    "\n",
    "                # Update Pygame screen\n",
    "                self.screen.fill((0, 0, 0))\n",
    "                self.draw_snake()\n",
    "                self.draw_pebble()\n",
    "                self.draw_score(total_reward)\n",
    "                self.draw_grid()\n",
    "                pygame.display.flip()\n",
    "\n",
    "                # Cap the frame rate\n",
    "                self.clock.tick(FPS)\n",
    "\n",
    "                # Exit the loop if the environment is no longer running\n",
    "                if not running:\n",
    "                    pygame.quit()\n",
    "                    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an instance for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MyDQNModel' object has no attribute 'conv_network'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Calculate the input size based on the state representation in the environment\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# input_size = len(EnvironmentGUIVersion().game_env.get_state()[0])\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m input_size \u001b[38;5;241m=\u001b[39m \u001b[43mEnvironmentGUIVersion\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgame_env\u001b[38;5;241m.\u001b[39mget_state()\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Assuming 4 possible actions (UP, DOWN, LEFT, RIGHT)\u001b[39;00m\n\u001b[0;32m      7\u001b[0m n_actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m  \n",
      "Cell \u001b[1;32mIn[10], line 23\u001b[0m, in \u001b[0;36mEnvironmentGUIVersion.__init__\u001b[1;34m(self, model, n_observations)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Use the provided model or create a new one\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# self.model = model if isinstance(model, MyDQNModel) else MyDQNModel(n_observations, self.n_actions)\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m model \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, MyDQNModel) \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mMyDQNModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mn_observations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_actions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_input_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_image_height\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_output_probs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Exploration-exploitation trade-off parameter\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon \u001b[38;5;241m=\u001b[39m EPSILON_START\n",
      "Cell \u001b[1;32mIn[6], line 8\u001b[0m, in \u001b[0;36mMyDQNModel.__init__\u001b[1;34m(self, n_observation, n_actions, n_input_channels, input_image_height, n_output_probs, conv_layer_sizes, conv_kernel_sizes, act_func_maxpool, dense_layer_sizes, act_func_dense, dropout)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28msuper\u001b[39m(MyDQNModel, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Calculate the output size after the convolutional layers\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_out_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_calculate_conv_out_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_observation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Initialize the convolutional network\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_network \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList()\n",
      "Cell \u001b[1;32mIn[6], line 61\u001b[0m, in \u001b[0;36mMyDQNModel._calculate_conv_out_size\u001b[1;34m(self, input_size)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_calculate_conv_out_size\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_size):\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;66;03m# dummy_input = torch.zeros((1, 1, *input_size), dtype=torch.float32)\u001b[39;00m\n\u001b[0;32m     60\u001b[0m     dummy_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, input_size, \u001b[38;5;241m1\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m---> 61\u001b[0m     conv_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convolutional_layers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdummy_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m conv_out\u001b[38;5;241m.\u001b[39mview(conv_out\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[6], line 65\u001b[0m, in \u001b[0;36mMyDQNModel._convolutional_layers\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_convolutional_layers\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_network\u001b[49m:\n\u001b[0;32m     66\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(layer, nn\u001b[38;5;241m.\u001b[39mMaxPool2d):\n\u001b[0;32m     67\u001b[0m             x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(layer(x))\n",
      "File \u001b[1;32mc:\\Users\\artorias961\\mambaforge\\envs\\ee5830\\lib\\site-packages\\torch\\nn\\modules\\module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1612\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1613\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1614\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1615\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'MyDQNModel' object has no attribute 'conv_network'"
     ]
    }
   ],
   "source": [
    "# Calculate the input size based on the state representation in the environment\n",
    "# input_size = len(EnvironmentGUIVersion().game_env.get_state()[0])\n",
    "input_size = EnvironmentGUIVersion().game_env.get_state().size(0)\n",
    "\n",
    "\n",
    "# Assuming 4 possible actions (UP, DOWN, LEFT, RIGHT)\n",
    "n_actions = 4  \n",
    "\n",
    "# Create an instance of the MyDQNModel class with the calculated input size and number of actions\n",
    "model = MyDQNModel(\n",
    "    n_observation=input_size,\n",
    "    n_actions=n_actions,\n",
    "    n_input_channels=1,  # Assuming grayscale images, adjust if needed\n",
    "    input_image_height=1,  # Adjust if needed\n",
    "    n_output_probs=n_actions  # Number of output probabilities should match the number of actions\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Cell (Combining everything)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [1, 12]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m gui \u001b[38;5;241m=\u001b[39m EnvironmentGUIVersion(model)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mgui\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 126\u001b[0m, in \u001b[0;36mEnvironmentGUIVersion.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# Episode loop\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;66;03m# Handle events outside the main loop\u001b[39;00m\n\u001b[1;32m--> 126\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_events\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[0;32m    128\u001b[0m     \u001b[38;5;66;03m# Exploration-exploitation strategy\u001b[39;00m\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m random\u001b[38;5;241m.\u001b[39mrandom() \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon:\n\u001b[0;32m    130\u001b[0m         \u001b[38;5;66;03m# Explore: Choose a random action\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[10], line 56\u001b[0m, in \u001b[0;36mEnvironmentGUIVersion.handle_events\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Get the action predicted by the model\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 56\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39margmax()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# Print the action predicted by the model\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms Action: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maction\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\artorias961\\mambaforge\\envs\\ee5830\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[6], line 30\u001b[0m, in \u001b[0;36mMyDQNModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;124;03mForward pass of the DQN model.\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \n\u001b[0;32m     26\u001b[0m \u001b[38;5;124;03m:param x: Input tensor representing the state of the environment.\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;124;03mreturn: Output tensor representing the Q-values for each possible action.\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Apply ReLU activation to the output of the first convolutional layer\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvolution_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Apply ReLU activation to the output of the second convolutional layer\u001b[39;00m\n\u001b[0;32m     33\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvolution_two(x))\n",
      "File \u001b[1;32mc:\\Users\\artorias961\\mambaforge\\envs\\ee5830\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\artorias961\\mambaforge\\envs\\ee5830\\lib\\site-packages\\torch\\nn\\modules\\conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\artorias961\\mambaforge\\envs\\ee5830\\lib\\site-packages\\torch\\nn\\modules\\conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [1, 12]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "gui = EnvironmentGUIVersion(model)\n",
    "gui.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ee5830",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
